# Values for configuring the deployment of TimescaleDB
# The charts README is at:
#    https://github.com/timescale/helm-charts/tree/main/charts/timescaledb-single
# Check out the various configuration options (administration guide) at:
#    https://github.com/timescale/helm-charts/blob/main/charts/timescaledb-single/docs/admin-guide.md

# Override the deployment namespace
namespaceOverride: ""

# TimescaleDB single helm chart configuration
timescaledb-single:
  # disable the chart if an existing TimescaleDB instance is used
  enabled: &dbEnabled true

  # override default helm chart image to use one with newer promscale_extension
  image:
    repository: timescale/timescaledb-ha
    tag: pg14.5-ts2.8.0-p1
    pullPolicy: IfNotPresent

  env:
    - name: TSTUNE_PROFILE
      value: promscale

  # create only a ClusterIP service
  loadBalancer:
    enabled: false
  # number or TimescaleDB pods to spawn (default is 3, 1 for no HA)
  replicaCount: 1
  # backup is disabled by default, enable it
  # if you want to backup timescaleDB to s3
  # you can provide the s3 details on tobs install
  # in the user prompt or you can set s3 details in the
  # env variables for the following keys:
  # PGBACKREST_REPO1_S3_BUCKET
  # PGBACKREST_REPO1_S3_ENDPOINT
  # PGBACKREST_REPO1_S3_REGION
  # PGBACKREST_REPO1_S3_KEY
  # PGBACKREST_REPO1_S3_KEY_SECRET
  backup:
    enabled: false
  # TimescaleDB PVC sizes
  persistentVolumes:
    data:
      size: 1500Gi
    wal:
      size: 200Gi
  ## TimescaleDB resource requests
  resources:
    requests:
      memory: 25Gi
      cpu: 10000m
    limit:
      cpu: 15000m
      memory: 30Gi

  # Enable Prometheus exporter for PostgreSQL server metrics.
  # https://github.com/prometheus-community/postgres_exporter
  prometheus:
    enabled: true
    image:
      repository: quay.io/prometheuscommunity/postgres-exporter
      tag: v0.11.1

  # Specifies whether PodMonitor for Prometheus operator should be created
  podMonitor:
    enabled: true

# Values for configuring the deployment of the Promscale
# The charts README is at:
#   https://github.com/timescale/helm-charts/tree/main/charts/promscale
promscale:
  enabled: true
  image:
    repository: timescale/promscale
    tag: 0.14.0
    pullPolicy: IfNotPresent
  # to pass extra args
  extraArgs:
    - "--metrics.high-availability=true"
    - "--tracing.async-acks"

  extraEnv:
    - name: "TOBS_TELEMETRY_INSTALLED_BY"
      value: "helm"
    - name: "TOBS_TELEMETRY_VERSION"
      value: "{{ .Chart.Version }}"
    - name: "TOBS_TELEMETRY_TRACING_ENABLED"
      value: "true"
    - name: "TOBS_TELEMETRY_TIMESCALEDB_ENABLED"
      value: *dbEnabled

  serviceMonitor:
    enabled: true

  connectionSecretName: "tobs-promscale-connection"

  # Promscale deployment resource requests
  resources:
    requests:
      # By default this should be enough for a cluster
      # with only a few pods
      memory: 50Gi
      cpu: 10000m

# Enabling Kube-Prometheus will install
# Grafana & Prometheus into tobs as they
# are part of Kube-Prometheus already
kube-prometheus-stack:
  enabled: false

# Enable OpenTelemetry Operator
# If using tobs CLI you can enable otel with --enable-opentelemetry flag
opentelemetry-operator:
  enabled: true
  manager:
    image:
      repository: ghcr.io/open-telemetry/opentelemetry-operator/opentelemetry-operator
      tag: v0.60.0
    resources:
      limits:
        cpu: 100m
        memory: 500Mi
      requests:
        cpu: 50m
        memory: 300Mi
    serviceMonitor:
      enabled: true
    prometheusRule:
      enabled: true
  instrumentation:
    pythonImage: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:0.32b0
    javaImage: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java:1.18.0
    nodejsImage: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:0.31.0
  collector:
    # The default otel collector that will be deployed by helm once
    # the otel operator is in running state
    config: |
      receivers:
        jaeger:
          protocols:
            grpc:
            thrift_http:

        otlp:
          protocols:
            grpc:
            http:

      exporters:
        logging:
        otlp:
          endpoint: "{{ .Release.Name }}-promscale.{{ .Release.Namespace }}.svc:9202"
          compression: none
          tls:
            insecure: true
        prometheusremotewrite:
          endpoint: "http://{{ .Release.Name }}-promscale.{{ .Release.Namespace }}.svc:9201/write"
          tls:
            insecure: true

      processors:
        batch:

      service:
        pipelines:
          traces:
            receivers: [jaeger, otlp]
            exporters: [logging, otlp]
            processors: [batch]
          metrics:
            receivers: [otlp]
            processors: [batch]
            exporters: [prometheusremotewrite]

